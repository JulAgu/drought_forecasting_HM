{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduce the model introspection results\n",
    "**This notebook has as prerequisite the successful execution of the experiment : [02-train_HM_final.py](../02-train_HM_final.py)**\n",
    "\n",
    "Thus, having the [models/HM/HM.pt](../models/HM/HM.pt) file is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juagudelo/HOMEdev/DF_HM\n"
     ]
    }
   ],
   "source": [
    "# Snippet to point to the project root directory\n",
    "# This is useful when you want to import modules from the project root directory\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from utilities import data_pretreatment_hm\n",
    "from toolbox import charge_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_weeks = 6\n",
    "hidden_dim = 490\n",
    "n_layers = 2\n",
    "ffnn_layers = 2\n",
    "dropout = 0.1\n",
    "lr = 7e-5\n",
    "epochs = 9\n",
    "clip = 5\n",
    "embed_dim = [3, 3, 3, 3, 3, 3, 3]\n",
    "embed_dropout = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the hyperparameters dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {}\n",
    "\n",
    "hp_name_list = [\"batch_size\", \"output_weeks\",\n",
    "                \"hidden_dim\", \"n_layers\",\n",
    "                \"ffnn_layers\", \"dropout\",\n",
    "                \"lr\", \"epochs\", \"clip\",\n",
    "                \"embed_dim\", \"embed_dropout\"]\n",
    "\n",
    "for idx, hp in enumerate([batch_size, output_weeks,\n",
    "                          hidden_dim, n_layers,\n",
    "                          ffnn_layers, dropout,\n",
    "                          lr, epochs, clip,\n",
    "                          embed_dim, embed_dropout]):\n",
    "    # Add the hyperparameters to the hyperparameters dictionary\n",
    "    hyperparameters_dict[hp_name_list[idx]] = hp\n",
    "\n",
    "hyperparameters_dict[\"ablation_TS\"] = False\n",
    "hyperparameters_dict[\"ablation_tabular\"] = False\n",
    "hyperparameters_dict[\"ablation_attention\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:40<00:00,  1.02s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 622.16it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 118.94it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 10351.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 119.15it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 10178.20it/s]\n"
     ]
    }
   ],
   "source": [
    "data = data_pretreatment_hm()\n",
    "X_tabular_train = data[\"X_tabular_train\"]\n",
    "X_tabular_cat_train = data[\"X_tabular_cat_train\"]\n",
    "X_time_train = data[\"X_time_train\"]\n",
    "y_target_train = data[\"y_target_train\"]\n",
    "X_tabular_valid = data[\"X_tabular_validation\"]\n",
    "X_tabular_cat_valid = data[\"X_tabular_cat_validation\"]\n",
    "X_time_valid = data[\"X_time_validation\"]\n",
    "y_target_valid = data[\"y_target_validation\"]\n",
    "valid_fips = data[\"valid_fips\"]\n",
    "X_tabular_test = data[\"X_tabular_test\"]\n",
    "X_tabular_cat_test = data[\"X_tabular_cat_test\"]\n",
    "X_time_test = data[\"X_time_test\"]\n",
    "y_target_test = data[\"y_target_test\"]\n",
    "test_fips = data[\"test_fips\"]\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train),\n",
    "    torch.tensor(X_tabular_train),\n",
    "    torch.tensor(X_tabular_cat_train),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]),\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid),\n",
    "    torch.tensor(X_tabular_valid),\n",
    "    torch.tensor(X_tabular_cat_valid),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(X_time_test),\n",
    "    torch.tensor(X_tabular_test),\n",
    "    torch.tensor(X_tabular_cat_test),\n",
    "    torch.tensor(y_target_test[:, :output_weeks]),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "NVIDIA T1000 8GB\n"
     ]
    }
   ],
   "source": [
    "model, device = charge_model(kind_of_model=\"HM\",\n",
    "                             model_path=\"models/HM/HM.pt\",\n",
    "                             hyperparams=hyperparameters_dict,\n",
    "                             dim_info= {\"static_dim\": X_tabular_train.shape[1],\n",
    "                                        \"n_tf\": X_time_train.shape[-1],\n",
    "                                        \"list_cat\": [len(np.unique(X_tabular_cat_train[:,i])) + 1 for i in range(X_tabular_cat_train.shape[1])]\n",
    "                                        },\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Inference: 100%|██████████| 20/20 [00:00<00:00, 343.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2477, 21) (2477, 7) (2477, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fips = []\n",
    "embeddings_all = []\n",
    "cat_data_all = []\n",
    "target_all = []\n",
    "# On extrait les embebbings sur l'ensemble de test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, static, catego ,y in tqdm(\n",
    "    test_loader,\n",
    "    desc=\" Inference\",\n",
    "    ):\n",
    "        val_h = tuple([each.data.to(device) for each in model.init_hidden(len(x), device)])\n",
    "        x, static, cat, y = x.to(device), static.to(device), catego.to(device), y.to(device)\n",
    "        # On obtient les embebbings pour chaque batch\n",
    "        embeddings = [emb(cat[:, i]) for i, emb in enumerate(model.embeddings)]\n",
    "        x_cat = torch.cat(embeddings, dim=1)\n",
    "        fips.append(static[:,14].cpu().numpy())\n",
    "        embeddings_all.append(x_cat.cpu().numpy())\n",
    "        cat_data_all.append(cat.cpu().numpy())\n",
    "        target_all.append(y.cpu().numpy())\n",
    "# Finalment on concatène les embebbings de tous les batchs\n",
    "fips = np.concatenate(fips)\n",
    "embeddings_all = np.concatenate(embeddings_all)\n",
    "cat_data_all = np.concatenate(cat_data_all)\n",
    "target_all = np.concatenate(target_all)\n",
    "print(embeddings_all.shape, cat_data_all.shape, target_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data resulting from the inference to make a t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame(embeddings_all)\n",
    "target = pd.DataFrame(target_all, columns=[f\"week_{i}\" for i in range(1, 7)])\n",
    "target = target.round().astype(int)\n",
    "cat_data = pd.DataFrame(cat_data_all, columns=[f\"SQ{i+1}\" for i in range(cat_data_all.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 361 nearest neighbors...\n",
      "[t-SNE] Indexed 2477 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 2477 samples in 0.117s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 2477\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 2477\n",
      "[t-SNE] Computed conditional probabilities for sample 2477 / 2477\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 39.122368\n",
      "[t-SNE] KL divergence after 1000 iterations: -0.402381\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=110, random_state=42, verbose = 1)\n",
    "X_embedded = tsne.fit_transform(df_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_colors = len(cat_data[\"SQ1\"].unique())\n",
    "color_map_25 = plt.cm.tab20(np.linspace(0, 1, num_colors))\n",
    "from matplotlib.colors import ListedColormap\n",
    "color_map_25 = ListedColormap(color_map_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.rc('font', **{'family':'serif', 'serif':['Computer Modern Roman']})\n",
    "params = {'backend': 'pdf',\n",
    "          'axes.labelsize': 22,\n",
    "          'font.size': 22,\n",
    "          'legend.fontsize': 16,\n",
    "          'xtick.labelsize': 18,\n",
    "          'ytick.labelsize': 18,\n",
    "          'text.usetex': True,\n",
    "          'axes.unicode_minus': True}\n",
    "mlp.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11.5, 10))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], label='Other Clusters', c=cat_data[\"SQ1\"], cmap=color_map_25, s=15)\n",
    "handles = [plt.Line2D([0], [0], marker='o',\n",
    "                      color='w',\n",
    "                      markerfacecolor=color_map_25(i),\n",
    "                      markersize=13) for i in range(num_colors)]\n",
    "plt.legend(handles=handles,\n",
    "           labels=[i+1 for i in list(np.sort(cat_data[\"SQ1\"].unique()))],\n",
    "           title = \"Nutrient \\n availability \\n scores\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/t-SNE_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention curve visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference over the lstm + softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predictions :: 100%|██████████| 20/20 [00:04<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "attention_all = []\n",
    "cat_data_all = []\n",
    "num_data_all = []\n",
    "target_all = []\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, static, catego ,y in tqdm(\n",
    "    valid_loader,\n",
    "    desc=\"Predictions :\",\n",
    "    ):\n",
    "        val_h = tuple([each.data.to(device) for each in model.init_hidden(len(x), device)])\n",
    "        x, static, cat, y = x.to(device), static.to(device), catego.to(device), y.to(device)\n",
    "        # Obtener los pesos de atención\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        lstm_out, _ = model.lstm(x, val_h)\n",
    "        attention_weights = torch.softmax(model.attention(lstm_out), dim=1)\n",
    "\n",
    "        attention_all.append(attention_weights.cpu().numpy())\n",
    "        cat_data_all.append(cat.cpu().numpy())\n",
    "        num_data_all.append(static.cpu().numpy())\n",
    "        target_all.append(y.cpu().numpy())\n",
    "\n",
    "attention_all = np.concatenate(attention_all)\n",
    "cat_data_all = np.concatenate(cat_data_all)\n",
    "num_data_all = np.concatenate(num_data_all)\n",
    "target_all = np.concatenate(target_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2455, 180, 1)\n"
     ]
    }
   ],
   "source": [
    "unique_attentions = np.unique(attention_all, axis=0)\n",
    "print(unique_attentions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_all = attention_all.reshape(-1, attention_all.shape[-2])\n",
    "attention_all = attention_all.transpose()\n",
    "att = pd.DataFrame(attention_all).reset_index(drop=False)\n",
    "att =att.melt(id_vars='index', value_name='attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.lineplot(att, x='index', y='attention')\n",
    "sns.despine()\n",
    "plt.xlabel(\"Days\")\n",
    "plt.xticks(range(0, 181, 20))\n",
    "plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "plt.ylabel(\"Mean attention weight\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/attention_weights.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
